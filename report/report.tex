\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\pagenumbering{gobble}
\usepackage[margin=0.5in]{geometry}

%\usepackage{geometry}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage{todonotes}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\setcounter{secnumdepth}{4}
%\usepackage{enumerate}% http://ctan.org/pkg/enumerate
\title{Computational Model of Peri-Personal Space}
\author{Joan Reyero}
%\date{\today}

\newtheorem*{theorem}{Theorem}


\bibliographystyle{apalike}
%\bibliography{bib}


\begin{document}

\maketitle

\section{Reinforcement learning models}

The behaviour of the participants was modelled using a reinforcement learning model, in which the value of the chosen stimulus $i$ was updates on trial $t$ after observing reward $r$ according to equation \ref{eq:0.0}. The initial values $V_0$ are set to 0.

\begin{equation}
    V_t^i = V_t^i + \epsilon + (r_t - V_t^i)
    \label{eq:0.0}
\end{equation}

The probability of choosing stimulus $A$ on trial $t$ is modelled by equation \ref{eq:0.1}

\begin{equation}
    P(\text{action } A | V_t, \beta) = \frac{\mathrm{exp}(\beta \times V_t^A)}{\mathrm{exp}(\beta \times V_t^A) + \mathrm{exp}(\beta \times V_t^B)}
    \label{eq:0.1}
\end{equation}


\subsection{Data exploration}

Before attempting to fit a model some data exploration was performed. Figure \ref{fig:2.1} shows the number of times each participant chose stimulus $A$, and the total number of rewards obtained. While participants who chose more times stimulus $A$ tend to have higher rewards, there is not a clear pattern observable in the figure. 

\begin{figure}[h!]
	\centering
	\hspace*{-0.6in}
	\includegraphics[width=1.2\linewidth]{figures/2.1.pdf}
	\caption{Plot of the total number of rewards and the number of times a participant chose stimulus $A$ for each participant.}
	\label{fig:2.1}
\end{figure}

The table below summarises the basic statistics for the studied variables. $A$ was chosen more than $50\%$ of the times, which is not surprising as, on average, choice $A$ had a higher probability of reward. The mean total rewards was 153.5, which is 64\% of the total.

The expected number of rewards if random choices were made can be obtained my

\[ \frac{\big(P(\mathrm{R} | A^1) + P(\mathrm{R} | B^1) + P(\mathrm{R} | A^2) + P(\mathrm{R} | B^2)\big) \times 100}{4} = 56.25 \]

Therefore, the expected number of rewards is lower than the average rewards obtained, which suggests that learning happened during the experiment.

\begin{center}
 \begin{tabular}{|c || c | c | c|} 
 \hline
  & Mean (out of 240) & Standard deviation & Range (out of 240) \\ [0.5ex] 
 \hline\hline
 Times $A$ was chosen & 141.0 & 20.31 & $[108, 200]$ \\ 
 \hline
 Total rewards & 153.5 & 6.63 & $[134,166]$ \\ [1ex] 
 \hline
\end{tabular}
\end{center}

\subsection{Simulated data exploration}

Figure \ref{fig:2.2} shows the average evolution of $V^A$, $V^B$ and $V^A - V^B$ for a simulation ran 100 times with 240 trials each. The trends clearly show that learning is happening with each probability switch, as when $V^A$ increases $V^B$ decreases and vice-versa. Moreover, it can be observer that when $P(\mathrm{R} | A) = 0.5 $ and $P(\mathrm{R} | B) = 0.75$ the learning happens slower than when $P(\mathrm{R} | A) = 0.75 $ and $P(\mathrm{R} | B) = 0.25$, as the difference in the probabilities is lower.

\begin{figure}[h!]
	\centering
	\hspace*{-0.4in}
	\includegraphics[width=1\linewidth]{figures/2.2.pdf}
	\caption{Plot of the average evolution of $V^A$ (top left), $V^B$ (bottom left) and $V^A - V^B$ (right), for a simulation ran 100 times with 240 trials. The golden dotted lines represent a switch in the probabilities.}
	\label{fig:2.2}
\end{figure}

The average number of rewards was 154.2.

\subsection{Parameter exploration}

For the parameter exploration, the values for the parameters were assigned:

\begin{description}
\item[$\epsilon_{\mathrm{values}}$:] linear space of 10 elements with range $[0, 1]$, 
\item[$\beta_{\mathrm{values}}$:] linear space of 15 elements with range $[1, 15]$.
\end{description}

Then, a simulation with 100 participants was performed for each pair of parameter values. 

\begin{figure}[h!]
	\centering
	\hspace*{-2.2in}
	\includegraphics[width=1.5\linewidth]{figures/2.3.pdf}
	\caption{Plot of the number of rewards for each learning rate and negative energy pair. The two subplots correspond to two different angles to improve visibility.}
	\label{fig:2.3}
\end{figure}

Figure \ref{fig:2.3} shows the amount of reward of each pair of parameters. The reward increases as the negative energy increases from 0 to 10, and it stabilises afterwards. The learning rate also increases the reward as it increases up to a threshold of around 0.7, then the reward decreases as the learning rate increases.

\subsection{Negative log likelihood}

The negative log likelihood (NNL), using the formula provided in the report, was computed for the first participant, and it was 52 to two significant figures. 

For the second participant, the NNL was 63.83.

\subsection{Model fitting and group comparison}

\subsubsection{Model fitting}
\label{sec:fit-model-0}

The model was fit using the function \texttt{scipy.minimize} with the \texttt{BFGS} minimisation method \footnote{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html}. $\epsilon = 0.5$ and $\beta = 5$ were used as starting parameters.

The minimisation algorithm encountered an overflow for most participants when running completely unconstrained. Therefore, the learning rate was constrained to being greater than 0, as there cannot be negative learning rates.

The mean learning rate (LR) found was 0.36, with a standard deviation of 0.123. The mean negative energy (NE) was 5.83, with a standard deviation of 1.25. 

\begin{figure}[h!]
	\centering
	\hspace*{-0.6in}
	\includegraphics[width=1.1\linewidth]{figures/2.4.pdf}
	\caption{Best-fit learning rate (top) and negative energy (bottom) for healthy and MDD participants. The participants with 0 values were filtered-out because the best-fits were outside the normal range of $(0,1)$ for the LR and $(1, 15)$ for the NE.}
	\label{fig:2.4}
\end{figure}

Figure \ref{fig:2.4} shows the best-fit LR an NE for each participant, with considerably more variability in the MDD participants than in the healthy. It can also be observed that participant 8 has a value of 0 for both the LR and NE, this is because a filtering function was implemented to remove from the best-fit parameters those participants who did not fall within the normal range. The zero values seen in the plot are simply to keep the number of the participants constant, they were not taken into account for the reported statistics. 

Furthermore, the Pearson correlation coefficient was computed for all participants, and for the healthy and MDD participants together. The results can be seen in the table below. The p-value represents the probability that that the result would have been found if the correlation coefficient was 0.

\begin{center}
 \begin{tabular}{|c || c | c |} 
 \hline
  & Pearson correlation coefficient & p-value  \\ [0.5ex] 
 \hline\hline
 Overall & -0.22 & 0.11 \\ 
 \hline
 Healthy & -0.30 & 0.23 \\
 \hline
 MDD & -0.13 & 0.46 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

There are no strong correlations between the best-fit LR and NE for any of the groups, which indicates that the optimisation algorithm functioned as expected, as high correlations would indicate anomalies in the data or the optimisation. 

\subsubsection{Group comparison}
\label{sec:t-model-0}
A two-sample T-test between the healthy and MDD groups was performed for both the LR and the EPS. The null hypothesis was that $\mu_{\mathrm{healthy}} = \mu_{\mathrm{MDD}}$. The degrees of freedom in both cases were 16 (as only 17 MDD patients were considered). The results of the T-statistic and the p-value can be seen in the table below:

\begin{center}
 \begin{tabular}{|c || c | c |} 
 \hline
  & T-statistic & p-value  \\ [0.5ex] 
 \hline\hline
 LR ($\epsilon$) & 1.33 & 0.19 \\
 \hline
 NE ($\beta$) & -0.66 & 0.51 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

There is only a 0.19 probability that the null hypothesis is true for the LR, therefore we can probably reject the null hypothesis and say that there is a difference between both means.

On the other hand, for the NE there is a 0.51 probability that the null hypothesis is true. Thus, we cannot accept nor reject the null hypothesis or establish a difference between groups.

Research about the effect of the LR has arrived to contradictory conclusions. \cite{chase2010approach} found a reduced learning rate in middle-aged MDD patients using a probabilistic selection task, which would be consistent with the findings in this report. However, \cite{gradin} found no difference in the LR between MDD patients and healthy controls, and \cite{dombrovski2010reward} found that there was only a difference between groups for the punishment LR, not the reward LR that is being considered in this report.

Moreover, \cite{series} found that the main impairments were the NE, which did not show as a differentiating factor in our findings, and a memory parameter that was not taken into account in our models.

\subsection{Parameter recovery}

For the parameter recovery 53 pairs of parameters were obtained randomly from a normal distribution. Then, 53 data-sets instances of 10 participants each were simulated. For each instance, the average best-fit parameters were computed.

\begin{figure}[h!]
	\centering
	\hspace*{-0.6in}
	\includegraphics[width=1.15\linewidth]{figures/2.5.0.pdf}
	\caption{Distribution of the LR (left) and the NE (right) randomly sampled from normal distributions.}
	\label{fig:2.5.0}
\end{figure}


Figure \ref{fig:2.5.0} shows the distribution of the starting LRs and NEs. There are no values outside the expected ranges, thus no values were re-sampled.

The Pearson correlation coefficient between the starting and the best-fit LRs is 0.56, with a p-value of $10^{-16}$ there is a strong correlation with a very small probability that there is no correlation.

For the NEs, the Pearson correlation coefficient is 0.89 with a p-value of $10^{-19}$: the correlation is even stronger with a negligent probability that there is no correlation. The same information is illustrated in figure \ref{fig:2.5.1}

\begin{figure}[h!]
	\centering
	\hspace*{-0.6in}
	\includegraphics[width=1.15\linewidth]{figures/2.5.1.pdf}
	\caption{Relationship between the randomly sampled starting parameters and the best-fit parameters between for the LR (left) and the NE (right). The golden line represents the line of best fit.}
	\label{fig:2.5.1}
\end{figure}

\subsection{Additional models}

Two additional models were considered to fit the data. The first model got rid of the negative energy parameter (equation \ref{eq:1.1} when calculating the probability and introduced a new parameter when calculating the reinforcement learning values: a reward sensitivity (RS) parameter (equation \ref{eq:1.0}). The second model used the same equations as the initial one, but it has initial reinforcement learning values other $V_0 = (V^A_0, V^B_0)$. 

\begin{equation}
    V_t^i = V_t^i + \epsilon + (\rho \times r_t - V_t^i)
    \label{eq:1.0}
\end{equation}

\begin{equation}
    P(\text{action } A | V_t) = \frac{\mathrm{exp}(V_t^A)}{\mathrm{exp}(V_t^A) + \mathrm{exp}(V_t^B)}
    \label{eq:1.1}
\end{equation}

\subsubsection{Model 1}

To fit the model 1 the starting parameters chosen were $\epsilon=0.5$, as in model 0, and $\rho=5.5$, as given in the course lectures.

The table below shows the Pearson correlation coefficient between the two fit parameters for all participants, healthy participants, and MDD participants. There is not a strong correlation between any, which, as discussed in section \ref{sec:fit-model-0}, was expected.

\begin{center}
 \begin{tabular}{|c || c | c | c| c |} 
 \hline
  & Pearson correlation coefficient & p-value  \\ [0.5ex] 
 \hline\hline
 Overall & -0.21 & 0.14  \\ 
 \hline
 Healthy & -0.13 & 0.48 \\
 \hline
 MDD & -0.33 & 0.48 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

\begin{figure}[h!]
	\centering
	\hspace*{-0.6in}
	\includegraphics[width=1.1\linewidth]{figures/2.6.1.pdf}
	\caption{Best-fit learning rate (top) and negative energy (bottom) for healthy and MDD participants. The participants with 0 values were filtered-out because the best-fits were outside the normal range of $(0,1)$ for the LR and $(1, 15)$ for the NE.}
	\label{fig:2.6.1}
\end{figure}

Figure \ref{fig:2.6.1} shows the best fit LR and RS for all participants. It can be observed that more participants had values outiside the normal range and had to be discarded than with model 0. There is a lot more variability between MDD than between healthy participants for the LR, which was to be expected from model 0. The distribution of the RS, however, seems to be very similar between groups. A two sample T-test as in section \ref{sec:t-model-0} was performed to get better insights about the group differences. The results can be seen in the table below.

\begin{center}
 \begin{tabular}{|c || c | c |} 
 \hline
  & T-statistic & p-value  \\ [0.5ex] 
 \hline\hline
 LR ($\epsilon$) & 1.33 & 0.19 \\
 \hline
 RS ($\rho$) & 0.31 & 0.76 \\ [1ex] 
 \hline
\end{tabular}
\end{center}


There is a statistically significant difference between the menans of the LR for healthy and MDD participants at an 80\% confidence level. However, there seems to be no difference in the RS between the groups. These results were surprising as research has found that when performing a probabilistic reward task to asses reward learning, participants with MDD had reduced reward sensitivity and hence performed worse \cite{rho} .

\subsubsection{Model 2}

To fit the model 2 the $\epsilon$ and $\beta$ parameters were 0.5 and 5.5 respectively, as in model 0. However, the starting reinforcement learning values were $(0.3, 0.1)$. $A$ was given a bigger starting value than $B$ because the there is a bias in the given probabilities towards choice $A$. 

The table below shows the Pearson correlation coefficient between the two fit parameters for all participants, healthy participants, and MDD participants. As with the previous two models, there are no strong correlations.

\begin{center}
 \begin{tabular}{|c || c | c | c| c |} 
 \hline
  & Pearson correlation coefficient & p-value  \\ [0.5ex] 
 \hline\hline
 Overall & -0.26 & 0.07  \\ 
 \hline
 Healthy & -0.13 & 0.46 \\
 \hline
 MDD & -0.34 & 0.16 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

\begin{figure}[h!]
	\centering
	\hspace*{-0.6in}
	\includegraphics[width=1.1\linewidth]{figures/2.6.1.pdf}
	\caption{Best-fit learning rate (top) and negative energy (bottom) for healthy and MDD participants. The participants with 0 values were filtered-out because the best-fits were outside the normal range of $(0,1)$ for the LR and $(1, 15)$ for the NE.}
	\label{fig:2.6.2}
\end{figure}

Figure \ref{fig:2.6.2} shows the best-fit parameters for all participants. We can, again, observe more variability between groups in the LR than in the NE, which seems to be uniform. The results of the two-sample T-test can be seen below.

\begin{center}
 \begin{tabular}{|c || c | c |} 
 \hline
  & T-statistic & p-value  \\ [0.5ex] 
 \hline\hline
 LR ($\epsilon$) & 1.51 & 0.14 \\
 \hline
 NE ($\beta$) & -0.5 & 0.61 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

As in the previous two models, there is a statistically significant difference between groups at an 86\% confidence level for the LR. There does not seem to be a difference for the NE.

\subsection{Model Comparison}

To perform model comparison the data was fit using the three models. Then, for each model the $AIC$ and $BIC$ scores were computed to see which model fit the data best. 

Each of the three models was fit, the negative log likelihood was computed, and the scores for each model were obtained using equations \ref{eq:aic} and \ref{eq:bic}.

\begin{equation}
\label{eq:aic}
AIC = \sum_{\forall \mathrm{ppts}} 2 \times NLL + 2 p = 2 p \times |\mathrm{ppts}|  + 2 \sum_\mathrm{\forall \mathrm{ppts}} NLL
\end{equation}

\begin{equation}
BIC = \sum_{\forall \mathrm{ppts}} 2 \times NLL + 2 \log(n) =  p \times \log(n) \times |\mathrm{ppts}|  + 2 \sum_\mathrm{\forall \mathrm{ppts}} NLL
\label{eq:bic}
\end{equation}

The results are shown in the table below.


\begin{center}
 \begin{tabular}{|c || c | c | c |} 
 \hline
  & Model 0 & Model 1 & Model 2  \\ [0.5ex] 
 \hline\hline
 $AIC$ & 8377 & 8633 & 8610 \\
 \hline
 $BIC$ & 33129 & 33385 & 33362 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

According to the material in lectures, there is a winning model if the difference in scores between models is $\>$ 10. However, since we have added scores for 53 participants, we would expect a difference between models $\> 530$. Model 0 has the lowest scores, but greatest difference between any Model 0 scores and any other score is always smaller than 300. 

From this we can conclude that Model 0 might be the best fitting model to the data, but we would need extra 

%\begin{theorem}
%The negative log likelihood for the actions of a participant will be the same using model 0 (equations \ref{eq:0.0} and %\ref{eq:0.1}) or model 1 (equations \ref{eq:1.0} and \ref{eq:1.1}). 
%\end{theorem}

%\begin{proof}



%\end{proof}
\newpage


\bibliography{bib}

\end{document}
